import os, wave, json, time, torch, soundfile as sf, numpy as np
from pathlib import Path
from tqdm import tqdm
from silero_vad import load_silero_vad, VADIterator
from module import asr, llm_qwen3o, tts, get_wav, api_qwen3o
class ConversationEngine:
    """
    Full-duplex Conversation Engine
    å®ç° LISTEN â†’ SPEAK å¾ªç¯çš„åŸºç¡€æ¡†æ¶
    æ¯ä¸€è½®è¾“å‡º listen/speak ä¸¤ä¸ªå­å—
    """

    def __init__(self, sample_rate=16000, window_size=256):
        # ========== å¸¸é‡ ==========
        self.SAMPLE_RATE = sample_rate #16kHz
        self.WINDOW_SIZE = window_size #256 samples (~16ms)
        self.FRAME_SEC = window_size / sample_rate # 256 / 16000 = 0.016s
        self.INTERRUPT_LIMIT = int(1.5 / self.FRAME_SEC) #1.5 / 0.016 = 94 frames

        # ========== çŠ¶æ€å˜é‡ ==========
        self.STATE = "LISTEN"       # å½“å‰çŠ¶æ€
        self.IN_SPEECH = False      # å½“å‰æ˜¯å¦å¤„äºä¸€æ®µè¯­éŸ³ä¸­
        self.BUFFER = []            # ç´¯ç§¯å¸§ç¼“å†²
        self.TURN_IDX = 0           # å…¨å±€è½®æ•°
        self.MEDIA_TIME = 0.0       # ç´¯è®¡éŸ³é¢‘æ—¶é—´ï¼ˆç§’ï¼‰
        self.FRAME_IDX = 0          # å¸§è®¡æ•°
        self.CURRENT_TURN = None    # å½“å‰è½®çš„ listen + speak æ•°æ®
        self.SILENCE_COUNTER = 0    # é™éŸ³è®¡æ•°å™¨
        self.INTERRUPT_COUNT = 0    # æ‰“æ–­å¸§è®¡æ•°
        self.FILE_NAME = None
        #endåå»¶ç»­ä¸€ä¸‹ï¼Œä¸è¦ç«‹åˆ»åˆ¤æ–­
        self.END_HOLD_SEC = 0.64
        self.END_HOLD_FRAMES = int(self.END_HOLD_SEC / self.FRAME_SEC) #0.64 / 0.016 = 40
        self.SILENCE_COUNTER = 0
        #æ‰“æ–­åå¤„ç†
        self.FROM_INTERRUPT = False
        #å†å²ä¸Šä¸‹æ–‡
        self.history = []


        # ========== è·¯å¾„ä¸æ¨¡å‹æ¥å£ ==========
        self.output_dir = None
        self.vad_model = load_silero_vad()
        self.vad_iterator = VADIterator(self.vad_model, sampling_rate=self.SAMPLE_RATE)


    def reset(self):
        self.STATE = "LISTEN"
        self.TURN_IDX = 0
        self.MEDIA_TIME = 0.0
        self.FRAME_IDX = 0
        self.CURRENT_TURN = None
        self.BUFFER.clear()
        self.history.clear()

    # -------------------------------------------------------
    def stream_audio(self, audio_path):
        """é€å¸§è¯»å–éŸ³é¢‘æµï¼ˆ16msä¸€å¸§ï¼‰"""
        with wave.open(str(audio_path), "rb") as wf:
            while True:
                data = wf.readframes(self.WINDOW_SIZE)
                if not data:
                    break
                chunk = np.frombuffer(data, dtype=np.int16).astype(np.float32) / 32768.0
                if len(chunk) < self.WINDOW_SIZE:
                    break
                yield chunk

    # -------------------------------------------------------
    def detect_vad_frame(self, chunk):
        """VAD æ£€æµ‹å‡½æ•°ï¼ˆè¿”å› start / end / Noneï¼‰"""
        if not hasattr(self, "_vad_buf"):
            self._vad_buf = np.zeros(0, dtype=np.float32)
        self._vad_buf = np.concatenate([self._vad_buf, chunk])
        if len(self._vad_buf) >= 2 * self.WINDOW_SIZE:
            tensor = torch.from_numpy(self._vad_buf[: 2 * self.WINDOW_SIZE])
            event = self.vad_iterator(tensor, return_seconds=True)
            self._vad_buf = np.zeros(0, dtype=np.float32)
            return event
        return None


    def process_user_segment(self, audio_buf):
        """å¯¹ä¸€æ®µå®Œæ•´çš„ç”¨æˆ·è¯­éŸ³æ‰§è¡Œ ASRâ†’LLMâ†’TTS å¹¶å†™å…¥ç»“æœ"""
        basename = self.output_dir.name
        turn_id = self.TURN_IDX
        #  # ========== ASR ==========
        # asr_start = time.perf_counter()
        # asr_text = asr(audio_buf)
        # asr_time = time.perf_counter() - asr_start
        # # ========== LLM ==========
        # llm_start = time.perf_counter()
        # decision = llm(asr_text)  # {"is_finished": bool, "reply": "..."}
        # llm_time = time.perf_counter() - llm_start

        # ========== api ==========
        api_start = time.perf_counter()
        # æ‹¼æ¥éŸ³é¢‘å¸§
        user_audio = np.concatenate(audio_buf) if isinstance(audio_buf, list) else audio_buf
        history_text = ""
        for turn in self.history[-3:]:  # ä¿ç•™æœ€è¿‘5è½®ï¼Œé˜²æ­¢promptå¤ªé•¿
            role = "ç”¨æˆ·" if turn["role"] == "user" else "åŠ©æ‰‹"
            history_text += f"{role}:{turn['content']}\n"
        listen_prompt = f'''
            é¦–å…ˆï¼Œå¦‚æœä½ è®¤ä¸ºç”¨æˆ·è¿™å¥è¯æ˜æ˜¾æ²¡æœ‰è¯´å®Œï¼Œè¯·åªè¾“å‡ºå­—ç¬¦ä¸²'continue'ã€‚ç„¶åç»™å‡ºè¿™æ®µéŸ³é¢‘çš„asrè½¬å½•,å¦‚æœä½ è®¤ä¸ºç”¨æˆ·æ˜æ˜¾è¯´å®Œï¼š
            ä½ æ˜¯ä¸€ä¸ªè‡ªç„¶èŠå¤©çš„è¯­éŸ³åŠ©æ‰‹ï¼Œè¦åƒæœ‹å‹ä¸€æ ·å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
            ä¸è¦åé—®ï¼Œä¹Ÿä¸è¦è§£é‡Šï¼Œä¸è¦è¾“å‡ºä»»ä½•æ ¼å¼è¯´æ˜ã€‚
            å¦‚æœç”¨æˆ·é—®åˆ°çš„å†…å®¹éœ€è¦ä½ ç¼–é€ ï¼Œæ¯”å¦‚ä¸çŸ¥é“ç­”æ¡ˆï¼Œä¹Ÿè¦è‡ªç„¶åœ°ç¼–é€ ä¸€ä¸ªåˆç†çš„å›ç­”ã€‚
            è¯­æ°”è¦å£è¯­åŒ–ã€ç®€çŸ­ï¼Œåƒæ—¥å¸¸èŠå¤©ã€‚å…è®¸å‡ºç°å¹»è§‰ã€‚

            ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š
            ç”¨æˆ·ï¼šæˆ‘ä¸Šæ¬¡åƒçš„é‚£å®¶ç«é”…åº—å«ä»€ä¹ˆæ¥ç€ï¼Ÿ
            åŠ©æ‰‹ï¼šè€ç¶ç«é”…å‘€ã€‚

            ä»¥ä¸‹æ˜¯éƒ¨åˆ†å†å²å¯¹è¯ï¼š
            {history_text}

            ç°åœ¨ï¼Œè¯·ç»§ç»­å›åº”ç”¨æˆ·çš„æœ€æ–°è¯­éŸ³ï¼š
        '''
        
        decision = llm_qwen3o(listen_prompt, user_audio)
    

        # âœ… å†™å…¥ä¸´æ—¶ wav å†å–‚ç»™ asr
        user_audio = np.concatenate(audio_buf) if isinstance(audio_buf, list) else audio_buf
        tmp_path = self.output_dir / f"{self.FILE_NAME}_turn{self.TURN_IDX}_input.wav"
        sf.write(tmp_path, user_audio, self.SAMPLE_RATE)
        user_text = asr(str(tmp_path)) # if "asr" in globals() else "<user audio>"
        self.history.append({"role": "user", "content": user_text})
        self.history.append({"role": "assistant", "content": decision})

        print(f"å†³ç­–ç»“æœ: {decision}")
        # exit(0)
        api_time = time.perf_counter() - api_start        
        if ("continue" not in decision.lower()) and ("ç»§ç»­" not in decision.lower()):

            #========== TTS ==========
            tts_path = self.output_dir / f"{basename}_r{turn_id}.wav"
            tts_start = time.perf_counter()
            tts_file = tts(decision, tts_path)
            tts_time = time.perf_counter() - tts_start

            audio_data, sr = sf.read(tts_file)
            tts_dur = len(audio_data) / sr
            # sys_start = self.MEDIA_TIME + asr_time + llm_time + tts_time
            sys_start = self.MEDIA_TIME + api_time + tts_time

            self.CURRENT_TURN = {
                "turn": turn_id,
                "user_end": round(self.MEDIA_TIME, 3),
                # "asr_time": round(asr_time, 3),
                # "llm_time": round(llm_time, 3),
                "api_time": round(api_time, 3),
                "tts_time": round(tts_time, 3),
                "tts_dur": round(tts_dur, 3),
                "sys_start": round(sys_start, 3),
                "tts_file": Path(tts_file).name
            }
            # self.write_turn()
            self.TURN_IDX += 1
            # print(self.CURRENT_TURN)
            self.STATE = "SPEAK"
            self.IN_SPEECH = False
            self.BUFFER.clear()
        else:
            # ---- æœªç»“æŸï¼Œç»§ç»­ç›‘å¬ ----
            self.STATE = "LISTEN"
            self.IN_SPEECH = True


    # -------------------------------------------------------
    def handle_listen(self, frame, event):
        """LISTEN çŠ¶æ€ï¼šæ£€æµ‹ç”¨æˆ·è¯­éŸ³ã€åˆ¤æ–­æ˜¯å¦è¯´å®Œã€å†³å®šæ˜¯å¦è¿›å…¥ SPEAK"""
        # --- 1. ç”¨æˆ·å¼€å§‹è¯´è¯ ---
        if event and "start" in event and not self.IN_SPEECH:
            self.IN_SPEECH = True
            self.BUFFER = [frame]
            return

        # --- 2. ç”¨æˆ·æ­£åœ¨è¯´è¯ ---
        if not self.IN_SPEECH:
            return  # ç”¨æˆ·æœªå‘è¨€ï¼Œç›´æ¥è·³è¿‡æœ¬å¸§

        self.BUFFER.append(frame)

        # --- 3. æ£€æµ‹è¯­éŸ³ç»“æŸ ---
        if event and "end" in event:
            self.SILENCE_COUNTER = 1
            # self.process_user_segment(self.BUFFER)
            return
        if self.SILENCE_COUNTER > 0:
        # å¦‚æœåœ¨é™éŸ³æœŸé—´å‡ºç°æ–°çš„ startï¼Œåˆ™ç»§ç»­æ¥ä¸Š buffer
            if event and "start" in event:
                self.SILENCE_COUNTER = 0
                return
            else:
                self.SILENCE_COUNTER += 1
                # è¾¾åˆ° 640 msï¼ˆEND_HOLD_FRAMESï¼‰åï¼Œç¡®è®¤ç»“æŸ
                if self.SILENCE_COUNTER >= self.END_HOLD_FRAMES:
                    self.SILENCE_COUNTER = 0
                    self.process_user_segment(self.BUFFER)
                    return

    def handle_speak(self, frame, event):
        """SPEAK çŠ¶æ€ï¼šæ£€æµ‹çŸ­æ‰“æ–­æˆ–é•¿æ‰“æ–­"""
        if event and "start" in event and not self.IN_SPEECH:
            self.IN_SPEECH = True
            self.interrupt_buf = [frame]
            self.INTERRUPT_COUNT = 1
            self.SILENCE_COUNTER = 0
            return

        if self.IN_SPEECH:
            self.interrupt_buf.append(frame)
            self.INTERRUPT_COUNT += 1

            # --- æ£€æµ‹åˆ°ç”¨æˆ·ç»“æŸè®²è¯ ---
            if event and "end" in event:
                self.SILENCE_COUNTER = 1
                return

            # --- é™éŸ³ç¡®è®¤é˜¶æ®µï¼ˆ640 ms å»¶è¿Ÿï¼‰---
            if self.SILENCE_COUNTER > 0:
                if event and "start" in event:
                    # 640ms å†…å‡ºç°æ–°è¯­éŸ³ â†’ ç»§ç»­æ¥ä¸Š
                    self.SILENCE_COUNTER = 0
                    return
                else:
                    self.SILENCE_COUNTER += 1
                    if self.SILENCE_COUNTER >= self.END_HOLD_FRAMES:
                        # âœ… ç¡®è®¤æ‰“æ–­ç»“æŸ
                        seg_audio = np.concatenate(self.interrupt_buf)
                        speak_prompt = (
                            "ä½ ç°åœ¨å¤„äº SPEAK çŠ¶æ€ï¼Œç”¨æˆ·åˆšæ‰åœ¨ä½ è¯´è¯æ—¶å‘å‡ºäº†ä¸€æ®µè¯­éŸ³ã€‚"
                            "è¯·æ ¹æ®è¯­ä¹‰åˆ¤æ–­ä»–æ˜¯å¦çœŸçš„æƒ³æ‰“æ–­ä½ ã€‚"
                            "å¦‚æœæ˜¯æ˜ç¡®çš„åé©³ã€å¦å®šã€æå‡ºé—®é¢˜ã€è¦æ±‚åœæ­¢ã€è¦æ±‚æ›´æ­£ç­‰ï¼Œè¿”å› 'interrupt'ï¼›"
                            "å¦‚æœåªæ˜¯é™„å’Œã€å›åº”ã€èµåŒæˆ–é¼“åŠ±ï¼ˆä¾‹å¦‚â€œå¥½çš„â€â€œçŸ¥é“äº†â€â€œè¯´å¾—å¥½â€â€œå—¯å—¯â€â€œè¡Œâ€ï¼‰ï¼Œ"
                            "è¯·è¿”å› 'continue'ã€‚"
                            "ä½ åªèƒ½è¿”å›è¿™ä¸¤ä¸ªå•è¯ä¹‹ä¸€ã€‚ä¸è¦è§£é‡Šã€ä¸è¦è¾“å‡ºå…¶å®ƒå†…å®¹ã€‚\n\n"

                            "ä»¥ä¸‹æ˜¯ä¸€äº›ç¤ºä¾‹ï¼š\n"
                            "ç”¨æˆ·ï¼šçŸ¥é“äº†ã€‚\nåŠ©æ‰‹ï¼šcontinue\n"
                            "ç”¨æˆ·ï¼šå¥½å¾—å¾ˆã€‚\nåŠ©æ‰‹ï¼šcontinue\n"
                            "ç”¨æˆ·ï¼šä½ è¯´å¾—çœŸæ£’ã€‚\nåŠ©æ‰‹ï¼šcontinue\n"
                            "ç”¨æˆ·ï¼šå—¯å—¯ï¼Œå¯¹ã€‚\nåŠ©æ‰‹ï¼šcontinue\n"
                            "ç”¨æˆ·ï¼šæˆ‘ä¸åŒæ„ä½ è¯´çš„ã€‚\nåŠ©æ‰‹ï¼šinterrupt\n"
                            "ç”¨æˆ·ï¼šä¸æ˜¯è¿™æ ·çš„ã€‚\nåŠ©æ‰‹ï¼šinterrupt\n"
                            "ç”¨æˆ·ï¼šä½ åˆ«è¯´äº†ã€‚\nåŠ©æ‰‹ï¼šinterrupt\n"
                            "ç”¨æˆ·ï¼šç­‰ä¸€ä¸‹ã€‚\nåŠ©æ‰‹ï¼šinterrupt\n\n"

                            "ç°åœ¨è¯·åˆ¤æ–­å½“å‰ç”¨æˆ·è¿™æ®µè¯­éŸ³çš„ç±»å‹ï¼Œåªè¿”å› 'interrupt' æˆ– 'continue'ï¼š"
                        )
                        intent = llm_qwen3o(speak_prompt, seg_audio)
                        print(f"å‡ºç°äº†æ‰“æ–­ï¼Œæ‰“æ–­æ„å›¾åˆ¤å®š: {intent}")

                        if "interrupt" in intent.lower():
                            self.CURRENT_TURN.setdefault("speak", {})["interrupt_time"] = round(self.MEDIA_TIME, 2)
                            self.write_turn()
                            # self.STATE = "LISTEN"
                            self.BUFFER = self.interrupt_buf.copy()
                            print("ğŸ” æ£€æµ‹åˆ°çŸ­æ‰“æ–­ï¼Œå¯åŠ¨æ–°ä¸€è½® listenâ†’speak")
                            self.process_user_segment(self.BUFFER)

                            # æ¸…ç†çŠ¶æ€
                            self.IN_SPEECH = False
                            self.interrupt_buf.clear()
                            self.INTERRUPT_COUNT = 0
                            self.SILENCE_COUNTER = 0
                            return

                        else:
                            # âŒ backchannel / ç»§ç»­è¯´ï¼šå¿½ç•¥æ‰“æ–­ï¼Œä¿æŒSPEAK
                            self.IN_SPEECH = False
                            self.interrupt_buf.clear()
                            self.INTERRUPT_COUNT = 0
                            self.SILENCE_COUNTER = 0
                            # ä¸åˆ‡æ¢ stateï¼Œç»§ç»­ SPEAK
                            return
            # 2.2 é•¿æ‰“æ–­ï¼šç´¯è®¡è¾¾åˆ°/è¶…è¿‡ 1.5sï¼Œæ— éœ€ç­‰å¾… endï¼Œç›´æ¥åˆ‡ LISTEN
            if self.SILENCE_COUNTER == 0 and self.INTERRUPT_COUNT >= self.INTERRUPT_LIMIT:
                print("âœ…å‡ºç°é•¿æ‰“æ–­ï¼Œåˆ‡æ¢åˆ°listenç»§ç»­å¬ï¼Œæ­£ç¡®å¼€å¯ç¬¬äºŒè½®")
                self.CURRENT_TURN.setdefault("speak", {})["interrupt_time"] = round(self.MEDIA_TIME, 2)
                self.write_turn()
                self.STATE = "LISTEN"
                self.BUFFER = self.interrupt_buf.copy()  # äº¤ç»™ä¸‹ä¸€è½®ç»§ç»­ç´¯è®¡
                self.IN_SPEECH = True
                self.interrupt_buf.clear()
                self.INTERRUPT_COUNT = 0
                return

        # 3) æœªæ£€æµ‹åˆ°ç”¨æˆ·å‘å£°ï¼šSPEAK æŒç»­ï¼ˆç¦»çº¿ä¸æ¨¡æ‹Ÿæ’­æ”¾ç»“æŸï¼‰
        return
    # -------------------------------------------------------
    def run(self, audio_path, output_dir):
        """ä¸»å¾ªç¯ï¼šé€å¸§æ‰§è¡Œ"""
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.FILE_NAME = audio_path.stem
        print(f"Processing file: {self.FILE_NAME}")
        for frame in self.stream_audio(audio_path):
            event = self.detect_vad_frame(frame)
            self.MEDIA_TIME = self.FRAME_IDX * self.FRAME_SEC
            self.FRAME_IDX += 1

            if self.STATE == "LISTEN":
                self.handle_listen(frame, event)
            elif self.STATE == "SPEAK":
                self.handle_speak(frame, event)

        # æ”¶å°¾ï¼šæœ€åä¸€è½®æœªå†™å…¥æ—¶å†™å…¥
        if self.CURRENT_TURN is not None:
            self.write_turn()
        self.vad_iterator.reset_states()

    # -------------------------------------------------------
    def write_turn(self):
        """å°†å½“å‰ turn å†™å…¥ JSONL"""
        jsonl_path = self.output_dir / f"{self.FILE_NAME}_r.jsonl"
        with open(jsonl_path, "a", encoding="utf-8") as f:
            f.write(json.dumps(self.CURRENT_TURN, ensure_ascii=False) + "\n")
        self.CURRENT_TURN = None


import shutil
import json
import numpy as np
import soundfile as sf

def process_folder(folder, save_root):
    """
    å¤„ç†å•ä¸ªå¯¹è¯æ–‡ä»¶å¤¹ï¼š
    - å¦‚æœæ–‡ä»¶å¤¹ä»¥ clean_ å¼€å¤´ï¼šç›´æ¥æŠŠ *_r1.wav æ”¹åä¸º *_output.wav å¤åˆ¶åˆ° save_rootã€‚
    - å¦åˆ™æŒ‰ JSONL æ‹¼æ¥å¤šä¸ª TTS wav ç”Ÿæˆ output.wavã€‚
    """
    # --- æƒ…å†µ 1ï¼šä»¥ clean_ å¼€å¤´ï¼Œç›´æ¥å¤åˆ¶ ---
    if folder.name.lower().startswith("clean_"):
        # æ‰¾å‡º _r1.wav æ–‡ä»¶
        r1_files = list(folder.glob("*_r0.wav"))
        if not r1_files:
            print(f"æœªæ‰¾åˆ° {folder} ä¸‹çš„ *_r0.wav æ–‡ä»¶")
            return
        
        src = r1_files[0]
        dst = save_root / f"{folder.name}_output.wav"
        save_root.mkdir(parents=True, exist_ok=True)
        shutil.copy(src, dst)
        print(f"å¤åˆ¶cleanéŸ³é¢‘: {src.name} â†’ {dst.name}")
        return

    # --- æƒ…å†µ 2ï¼šæ™®é€šæ–‡ä»¶å¤¹ï¼ŒæŒ‰ JSONL æ‹¼æ¥ ---
    jsonl = folder / f"{folder.name}_r.jsonl"
    if not jsonl.exists():
        print(f"JSONL ä¸å­˜åœ¨: {jsonl}")
        return

    data = [json.loads(l) for l in open(jsonl, encoding="utf-8") if l.strip()]
    data.sort(key=lambda x: x["sys_start"])

    segs, cur_len, sr = [], 0, 16000
    for s in data:
        wav, sr = sf.read(folder / s["tts_file"])
        if wav.ndim > 1:
            wav = wav[:, 0]
        pad = int(max(0.0, s["sys_start"] - cur_len) * sr)
        if pad > 0:
            segs.append(np.zeros(pad, dtype=wav.dtype))
            cur_len += pad / sr

        cut = None
        if "interrupt_time" in s and s["sys_start"] + s["tts_dur"] > s["interrupt_time"]:
            cut = int(max(0.0, s["interrupt_time"] - s["sys_start"]) * sr)
        if cut is not None:
            wav = wav[:cut]

        segs.append(wav)
        cur_len += len(wav) / sr

    if segs:
        save_root.mkdir(parents=True, exist_ok=True)
        out = save_root / f"{folder.name}_output.wav"
        sf.write(out, np.concatenate(segs), sr)
        print(f"æ‹¼æ¥ç”Ÿæˆ: {out}")


def main():
    exp_name = "exp1"
    data_lang = "dev_zh"
    out_lang = "medium_zh"
    category_dev = ["Pause Handling"]

    data_root = Path("exp") / exp_name / "dev" / data_lang
    output_root = Path("exp") / exp_name / "medium" / out_lang

    if output_root.exists():
        for category in category_dev:
            cat_dir = output_root / category
            if cat_dir.exists():
                shutil.rmtree(cat_dir)  # ç›´æ¥åˆ é™¤æ•´ä¸ªç±»åˆ«ç›®å½•
                print(f"âš ï¸ æ¸…ç©ºè¾“å‡ºå­ç›®å½•: {cat_dir}")

    else:
        output_root.mkdir(parents=True, exist_ok=True)
        
    engine = ConversationEngine()

    for category in category_dev:
        wav_path = data_root / category         # åŸå§‹æ•°æ®åŒº
        out_path = output_root / category       # ç”ŸæˆåŒº
        out_path.mkdir(parents=True, exist_ok=True)

        wav_files = get_wav(wav_path, "all")

        for wav in tqdm(wav_files, desc=f"Processing {category}"):
            wav_file = wav_path / wav
            # if wav_file.stem != "0005_0019_add":
            #     continue
            output_dir = out_path / wav_file.stem
            output_dir.mkdir(parents=True, exist_ok=True)
            
            engine.reset()      
            engine.run(wav_file, output_dir)

            jsonl_path = output_dir / f"{wav_file.stem}_r.jsonl"
            if not jsonl_path.exists():
                raise RuntimeError(f"æœªç”Ÿæˆç»“æœæ–‡ä»¶: {jsonl_path}")

            lines = [l.strip() for l in open(jsonl_path, "r", encoding="utf-8") if l.strip()]
            print(f"{wav_file.name} å®Œæˆ, å…± {len(lines)} è½®å¯¹è¯")

            # è¾“å‡ºç»“æœå†™å›åŸå§‹æ•°æ®ç›®å½•ï¼ˆä½ çš„è¯„æµ‹åŒºï¼‰
            process_folder(output_dir, wav_path)

if __name__ == "__main__":
    main()
